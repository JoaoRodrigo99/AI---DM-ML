{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy.stats\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sb\n",
    "\n",
    "# Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Other\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"../archive/rating_complete.csv\")\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = pd.read_csv(\"../archive/anime.csv\")\n",
    "anime_df[\"anime_id\"] = anime_df[\"MAL_ID\"]\n",
    "anime_df = anime_df.drop(columns={\"MAL_ID\",\"English name\",\"Aired\",\"Premiered\",\"Producers\",\"Licensors\",\"Studios\",\"Source\",\"Rating\",\"Ranked\", \"Japanese name\"})\n",
    "anime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(ratings, anime_df, on=\"anime_id\", how=\"inner\")\n",
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=0.001)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_recommenders as tfrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns={\"Name\",\"Genres\",\"Duration\",\"Type\"})\n",
    "# df_one_hot = pd.get_dummies(df)\n",
    "# df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "# df[\"Score\"]\n",
    "df = df.drop(columns={\"Score\", \"Episodes\", \"Score-10\",\"Score-9\",\"Score-8\",\"Score-7\",\"Score-6\",\"Score-5\",\"Score-4\",\"Score-3\",\"Score-2\",\"Score-1\"})\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.title('Correlation of Movie Features\\n', fontsize=18, weight=600, color='#333d29')\n",
    "sns.heatmap(df.corr(), annot=True, cmap=['#004346', '#036666', '#06837f', '#02cecb', '#b4ffff', '#f8e16c', '#fed811', '#fdc100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'].unique().astype(str)\n",
    "# map\n",
    "# tf.convert_to_tensor(df['user_id'].unique())\n",
    "\n",
    "df_NN = df.drop(columns={\"Popularity\", \"Members\", \"Favorites\", \"Watching\", \"Completed\", \"On-Hold\", \"Dropped\", \"Plan to Watch\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_NN[\"rating\"]\n",
    "# X = df_NN.loc[:, df.columns != \"rating\"] # All columns besides 'charges'\n",
    "X = df_NN.drop(columns={\"rating\"})\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "len(X), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_ds = tf.data.Dataset.from_tensor_slices(df_NN[\"anime_id\"].values)\n",
    "# list(dataset.as_numpy_iterator())\n",
    "# target = df_NN.pop('rating')\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(dict(df_NN))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(df_NN[['user_id','anime_id',\"rating\"]].values.astype('int32'))\n",
    "dataset = dataset.map(lambda x: {\"user_id\": x[0],\"anime_id\": x[1],\"rating\": x[2]})\n",
    "\n",
    "unique_anime_ids = np.unique(np.concatenate(list(tf.data.Dataset.from_tensor_slices(df_NN[['anime_id']].values.astype('int32')).batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(tf.data.Dataset.from_tensor_slices(df_NN[['user_id']].values.astype('int32')).batch(1_000))))\n",
    "\n",
    "for i in dataset:\n",
    "    print(i[\"anime_id\"])\n",
    "    break\n",
    "\n",
    "# print(len(unique_anime_ids),len(unique_user_ids))\n",
    "# len(tf.convert_to_tensor(df['user_id'].unique())),len(tf.convert_to_tensor(df['anime_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with embeddings\n",
    "\n",
    "user_lookup = tf.keras.layers.StringLookup(vocabulary=tf.convert_to_tensor(df['user_id'].unique().astype(str)), mask_token=None)\n",
    "user_embedding = tf.keras.layers.Embedding(input_dim=user_lookup.vocabulary_size(), output_dim=32)\n",
    "\n",
    "\n",
    "user_model = tf.keras.Sequential([user_lookup, user_embedding])\n",
    "\n",
    "movie_lookup = tf.keras.layers.StringLookup(vocabulary=tf.convert_to_tensor(df['anime_id'].unique().astype(str)), mask_token=None)\n",
    "movie_embedding = tf.keras.layers.Embedding(input_dim=movie_lookup.vocabulary_size(), output_dim=32)\n",
    "\n",
    "movie_model = tf.keras.Sequential([movie_lookup, movie_embedding])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, use_timestamps):\n",
    "    super().__init__()\n",
    "\n",
    "    self._use_timestamps = use_timestamps\n",
    "\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.IntegerLookup(vocabulary=df['user_id'].unique().astype('int32')),\n",
    "        # tf.keras.layers.StringLookup(\n",
    "        #     vocabulary=tf.convert_to_tensor(df['user_id'].unique().astype(str)), mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(df[\"user_id\"].unique()) + 1, 32),\n",
    "    ])\n",
    "\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if not self._use_timestamps:\n",
    "      return self.user_embedding(inputs[\"user_id\"])\n",
    "\n",
    "    return tf.concat([\n",
    "        self.user_embedding(inputs[\"user_id\"]),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    max_tokens = 10_000\n",
    "\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      # tf.keras.layers.StringLookup(\n",
    "      #     vocabulary=tf.convert_to_tensor(df['anime_id'].unique().astype(str)), mask_token=None),\n",
    "      tf.keras.layers.IntegerLookup(vocabulary=df['anime_id'].unique().astype('int32')),\n",
    "      tf.keras.layers.Embedding(len(df[\"anime_id\"].unique()) + 1, 32)\n",
    "    ])\n",
    "\n",
    "    # self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    #     max_tokens=max_tokens)\n",
    "\n",
    "    self.title_text_embedding = tf.keras.Sequential([\n",
    "      # self.title_vectorizer,\n",
    "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    ])\n",
    "\n",
    "    # self.title_vectorizer.adapt(movies)\n",
    "\n",
    "  def call(self, titles):\n",
    "    return tf.concat([\n",
    "        self.title_embedding(titles),\n",
    "        # self.title_text_embedding(titles),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, use_timestamps):\n",
    "    super().__init__()\n",
    "    self.query_model = tf.keras.Sequential([\n",
    "      UserModel(False),\n",
    "      tf.keras.layers.Dense(64)\n",
    "    ])\n",
    "    self.candidate_model = tf.keras.Sequential([\n",
    "      MovieModel(),\n",
    "      tf.keras.layers.Dense(64)\n",
    "    ])\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "        metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=animes_ds.batch(128).map(self.candidate_model),\n",
    "        ),\n",
    "    )\n",
    " \n",
    "  def compute_loss(self, features, training=False):\n",
    "    # We only pass the user id and timestamp features into the query model. This\n",
    "    # is to ensure that the training inputs would have the same keys as the\n",
    "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "    # error when loading the query model after saving it.\n",
    "    # print(\"FEATURES\\n\" , features)\n",
    "    query_embeddings = self.query_model({\n",
    "        \"user_id\": features[\"user_id\"],\n",
    "        # \"timestamp\": features[\"timestamp\"],\n",
    "    })\n",
    "    movie_embeddings = self.candidate_model(features[\"anime_id\"])\n",
    "\n",
    "    return self.task(query_embeddings, movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "shuffled = dataset.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserModel.user_embedding(\"29806\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "model = MovielensModel(use_timestamps=False)\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam()) # worse (maybe bc default lr 0.01 vs 0.1?)\n",
    "\n",
    "model.fit((cached_train), epochs=3)\n",
    "\n",
    "train_accuracy = model.evaluate(\n",
    "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "test_accuracy = model.evaluate(\n",
    "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "\n",
    "print(f\"Top-100 accuracy (train): {train_accuracy:.2f}.\")\n",
    "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class AnimeModel(tfrs.models.Model):\n",
    "    def __init__(self) -> None  :\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_dimension = 64\n",
    "\n",
    "        self.anime_layers: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=tf.convert_to_tensor(df['anime_id'].unique().astype(str)), mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(df['anime_id'].unique()) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        self.user_layers: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=tf.convert_to_tensor(df['user_id'].unique().astype(str)), mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(df['user_id'].unique()) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        self.rating_layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # Tasks\n",
    "\n",
    "        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            loss = tf.keras.losses.MeanSquaredError(),\n",
    "            metrics = [tf.keras.metrics.RootMeanSquaredError()],\n",
    "        )\n",
    "\n",
    "        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "            metrics = tfrs.metrics.FactorizedTopK(candidates=df['user_id'].unique().astype(str).batch(128).map(self.anime_layers))\n",
    "        )\n",
    "        \n",
    "        # Loss weights\n",
    "        self.rating_weight = 1\n",
    "        self.retrieval_weight = 1\n",
    "\n",
    "    def call(self, features) -> tf.Tensor:\n",
    "            \n",
    "        user_embeddings = self.user_layers(features[\"userId\"])\n",
    "\n",
    "        anime_embeddings = self.anime_layers(features[\"anime_id\"])\n",
    "\n",
    "        return (\n",
    "            user_embeddings,\n",
    "            anime_embeddings,\n",
    "            self.rating_layer(tf.concat([user_embeddings, anime_embeddings], axis=1)),\n",
    "        )\n",
    "    \n",
    "\n",
    "    def compute_loss(self, features, training=False) -> tf.Tensor:\n",
    "        ratings = features.pop(\"rating\")\n",
    "\n",
    "        user_embeddings, anime_embeddings, rating_predictions = self(features)\n",
    "\n",
    "        # We compute the loss for each task.\n",
    "        rating_loss = self.rating_task(\n",
    "            labels=ratings,\n",
    "            predictions=rating_predictions,\n",
    "        )\n",
    "        retrieval_loss = self.retrieval_task(user_embeddings, anime_embeddings)\n",
    "\n",
    "        # And combine them using the loss weights.\n",
    "        return (self.rating_weight * rating_loss\n",
    "                + self.retrieval_weight * retrieval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AnimeModel()\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "# cached_train = X_train.shuffle(100_000).batch(1_000).cache()\n",
    "# cached_test = X_test.batch(1_000).cache()\n",
    "\n",
    "# model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_movie(user, top_n=3):\n",
    "    # Create a model that takes in raw query features, and\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "    # recommends movies out of the entire movies dataset.\n",
    "    index.index_from_dataset(\n",
    "      tf.data.Dataset.zip((df['anime_id'].unique().astype(str).batch(100), df['anime_id'].unique().astype(str).batch(100).map(model.movie_model)))\n",
    "    )\n",
    "\n",
    "    # Get recommendations.\n",
    "    _, titles = index(tf.constant([str(user)]))\n",
    "    \n",
    "    print('Top {} recommendations for user {}:\\n'.format(top_n, user))\n",
    "    for i, title in enumerate(titles[0, :top_n].numpy()):\n",
    "        print('{}. {}'.format(i+1, title.decode(\"utf-8\")))\n",
    "\n",
    "def predict_rating(user, movie):\n",
    "    trained_movie_embeddings, trained_user_embeddings, predicted_rating = model({\n",
    "          \"userId\": np.array([str(user)]),\n",
    "          \"original_title\": np.array([movie])\n",
    "      })\n",
    "    print(\"Predicted rating for {}: {}\".format(movie, predicted_rating.numpy()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_4.predict(X_test)\n",
    "# print(history)\n",
    "# model_4.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train.median(), y_train.mean())\n",
    "# print(y_test.median(), y_test.mean())\n",
    "# print(y_pred.median(), y_pred.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history (also know as a loss curve or a training curve)\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X_train[\"Completed\"], y_train, c=\"b\", label=\"train data\")\n",
    "plt.scatter(X_test[\"Completed\"], y_test, c='g', label=\"test data\")\n",
    "plt.scatter(X_test[\"Completed\"], tf.squeeze(y_pred), c=\"r\", label=\"predictions\")\n",
    "plt.ylim(-1, 15)\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
